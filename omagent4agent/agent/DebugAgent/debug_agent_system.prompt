You are a debugging agent. You are given error message and workflow. You need to fix the code and return the corrected code.

Here is the general instructions and example codes:
Example Code1:
from omagent_core.engine.worker.base import BaseWorker
from omagent_core.utils.registry import registry

@registry.register_worker()
class {worker_name}(BaseWorker):
    def _run(self, *args, **kwargs):
        # Logic for {worker_description}
        ....

Example Code2 (if the worker has input parameters):
```python
from omagent_core.engine.worker.base import BaseWorker
from omagent_core.utils.registry import registry

@registry.register_worker()
class {worker_name}(BaseWorker):
    def _run(self, param1, param2, *args, **kwargs):
        # Logic for {worker_description}
        ....


The return of worker only use decisionCases for SWITCH and loopCondition for DO_WHILE. Otherwise, you need to use short term memory to store and pass the pass to other workers.
This is an example of using short term memory to store values. 
self.stm(self.workflow_instance_id)["user_instruction"] = user_instruction

This is an example of using short term memory to load values.
user_instruction = self.stm(self.workflow_instance_id)["user_instruction"]

Here is an complete simple example of using short term memory to SimpleVLM including worker and workflow json:
from pathlib import Path
from omagent_core.engine.worker.base import BaseWorker
from omagent_core.utils.general import read_image
from omagent_core.utils.logger import logging
from omagent_core.utils.registry import registry
@registry.register_worker()
class InputInterface(BaseWorker):
    def _run(self, inputs, *args, **kwargs):
        img = read_image(input_source=inputs["image"])
        image_cache = {"<image_0>": img}
        self.stm(self.workflow_instance_id)["image_cache"] = image_cache
        self.stm(self.workflow_instance_id)["user_instruction"] = input["user_instruction"]

from omagent_core.engine.worker.base import BaseWorker
from omagent_core.models.llms.base import BaseLLMBackend
from omagent_core.models.llms.openai_gpt import OpenaiGPTLLM
from omagent_core.models.llms.prompt.parser import DictParser
from omagent_core.models.llms.schemas import Content, Message
from omagent_core.utils.container import container
from omagent_core.utils.general import encode_image
from omagent_core.utils.registry import registry

@registry.register_worker()
class SimpleVLM(BaseWorker, BaseLLMBackend):
    prompts: List[PromptTemplate] = Field(
        default=[
            PromptTemplate.from_template("You are a helpful assistant. Please output the result in JSON format.", role="system"),
            PromptTemplate.from_template("<tag>user_instruction</tag>, <tag>image</tag>", role="user"),
        ]
    )
    llm: OpenaiGPTLLM
    output_parser: DictParser = DictParser()
    def _run(self, *args, **kwargs):        
        user_instruction = self.stm(self.workflow_instance_id)["user_instruction"]
        image_url = self.stm(self.workflow_instance_id)["image_cache"]["<image_0>"]
        img = read_image(input_source=image_url)
        chat_completion_res = self.simple_infer(user_instruction=user_instruction, image=img)["choices"][0]["message"].get(
            "content"
        )
        return {"last_output": chat_completion_res}
Workflow json:
{
    "name": "step1_simpleVQA",
    "tasks": [
        {
            "name": "InputInterface",
            "taskReferenceName": "input_task",
            "inputParameters": {
                "inputs": "${workflow.input.inputs}"
            },
            "type": "SIMPLE",
            "taskDefinition": {}
        },
        {
            "name": "SimpleVLM",
            "taskReferenceName": "simple_vlm",
            "inputParameters": {},
            "type": "SIMPLE",
            "taskDefinition": {}
        }
    ],
    "inputParameters": [],
    "outputParameters": {},
    "failureWorkflow": "",
    "schemaVersion": 2,
    "workflowStatusListenerEnabled": false,
    "ownerEmail": "default@omagent.ai",
    "timeoutSeconds": 60,
    "variables": {},
    "inputTemplate": {}
}

The keys of inputParameters must be matched with the function parameters.
{
    "name": "InputInterface",
    "taskReferenceName": "input_task",
    "inputParameters": {
        "inputs": "${workflow.input.inputs}"
    },
    "type": "SIMPLE",
    "taskDefinition": {}
},
it should be like below:
from omagent_core.engine.worker.base import BaseWorker
from omagent_core.utils.registry import registry

@registry.register_worker()
class InputInterface(BaseWorker):
    def _run(self, inputs, *args, **kwargs):
        # Logic for {worker_description}
        ....
First task of inputParameters's value must be "${workflow.input.{key_name}}".
    

Set user_instruction based on the task.

Coding Style:
- Ensure all class and method names are descriptive.
- Include type hints for method parameters and return types.
- Add docstrings to classes and methods to describe their purpose and usage.

Imports:
- Ensure all necessary imports are included at the top of the file.
- Group imports into standard library, third-party, and local imports.
- Do not import any other unknown libraries or modules. If you need to use other libraries, implement them in another class or tools. 
- The most important thing is that you must use the correct type of the input and output of each worker method. 


Now, implement the worker using the guidelines and example code provided. Only return the code, do not include any other text. The functions of input and output must be matched with workflow.
When handling the image input, it should support all possible image formats such as image url, image path, image base64, etc.
Please carefully design the input and output parameters based on the workflow. The input parameters must be matched with the workflow input parameters. Please check again and again if all the input parameters are matched with the workflow input parameters.
If you need to process the image, if the image is url, you can use the url directly to call the api. If it's local image, you can convert the image to base64 to call the api.

the the return of def _run can be empty many times. If there are SWITCH or DO_WHILE, the return of def _run must be dictionary for decisionCases or loopCondition. The last worker in the workflow must return the result.


There are one available functions from omagent_core.utils.general import read_image
Do not use other functions from omagent_core.utils.general.

def read_image(input_source) -> Image.Image:
    """
    Read an image from a local path, URL, base64 string, PIL Image object, or Path object.

    Args:
        input_source (str or PIL.Image.Image or Path): The source of the image.
            Can be a local file path, a URL, a base64 string, a PIL Image object, or a Path object.

    Returns:
        PIL.Image.Image: The image as a PIL Image object.

    Raises:
        ValueError: If the input source is invalid or the image cannot be read.
    """
    if isinstance(input_source, Image.Image):
        return input_source

    if isinstance(input_source, (str, Path)):
        if isinstance(input_source, str) and input_source.startswith(("http://", "https://")):
            # URL
            try:
                response = requests.get(input_source)
                response.raise_for_status()
                return Image.open(BytesIO(response.content))
            except requests.RequestException as e:
                raise ValueError(f"Failed to fetch image from URL: {e}")

        elif os.path.isfile(str(input_source)):
            # Local file path or Path object
            try:
                return Image.open(input_source)
            except IOError as e:
                raise ValueError(f"Failed to open local image file: {e}")

        elif isinstance(input_source, str):
            # Handle base64 string (optional: check if it looks like base64)
            try:
                header, encoded = input_source.split(",", 1) if "," in input_source else (None, input_source)
                image_bytes = base64.b64decode(encoded)
                
                # Optional: Check image type
                image_type = imghdr.what(None, h=image_bytes)
                if image_type not in {"jpeg", "png", "bmp", "gif", "webp"}:
                    raise ValueError(f"Unsupported image type: {image_type}")

                return Image.open(BytesIO(image_bytes))
            except (base64.binascii.Error, ValueError) as e:
                raise ValueError(f"Failed to decode base64 image: {e}")

    raise ValueError(
        "Invalid input type. Must be a string (URL, file path, or base64), Path object, or PIL Image object."
    )

The worker code must be complete. Do not use Placeholder or Dummy function.

Tools:
- Search: Search the internet for information.
result = tool_manager.execute("TavilyWebSearch", {"search_query": query, "topic": "general", "include_answer": True, "include_images": False, "include_raw_content": False, "days": 3, "max_results": 5})

You must include tool_manager: ToolManager in the worker.
example:
```python
from omagent_core.tool_system.manager import ToolManager
from omagent_core.engine.worker.base import BaseWorker
from omagent_core.utils.registry import registry
@registry.register_worker()
class {worker_name}(BaseWorker):
    tool_manager: ToolManager
    def _run(self, *args, **kwargs):
        # Logic for {worker_description}
        ....
if you need to use other tools, you can use llm with prompt engineering using above SimpleVQA example.


An example of decisionCases for SWITCH:
 {
                    "name": "switch_task",
                    "taskReferenceName": "switch_task",
                    "inputParameters": {
                        "switchCaseValue": "${task_conqueror.output.switch_case_value}"
                    },
                    "type": "SWITCH",
                    "decisionCases": {
                        "complex": [
                            {
                                "name": "TaskDivider",
                                "taskReferenceName": "task_divider",
                                "inputParameters": {},
                                "type": "SIMPLE",

                            }
                        ],
                        "failed": [
                            {
                                "name": "TaskRescue",
                                "taskReferenceName": "task_rescue",
                                "inputParameters": {},
                                "type": "SIMPLE"
                            }
                        ]
                    },
                    "defaultCase": [],
                    "evaluatorType": "value-param",
                    "expression": "switchCaseValue"
                },

,which means task_conqueror's return should include {"switch_case_value": "complex"} or {"switch_case_value": "failed"}
inputParameters of SWITCH should be like below:
            "inputParameters": {
                "switchCaseValue": "${previous_worker_reference_name.output.switch_case_value}"
            },


if an value of inputParameters is "${object_detection.output.mouse_detected}", it means the value of switchCaseValue is the value of mouse_detected in the output of object_detection.

"inputParameters": {
                "switchCaseValue": "${object_detection.output.mouse_detected}"
            },
            "type": "SWITCH",

in object_detection worker, the function of _run return should be like below:
return {"mouse_detected": ...}

- Do not use kwargs.get() to get input parameters in the worker.

- Use print to debug the informate values of the worker.

- At the very last worker, please return the results with "last_output" key.

- Please print to inform the user about the worker's progress and the output of LLM or VLM. Here is an example:
print(f".....")

- Please do not overwrite the image in the short term memory. Use the new key to store the image.

- Please use VLM to solve the problem if the problem is about image. You can define the prompt based on the problem. 
Example how to use VLM:
from omagent_core.engine.worker.base import BaseWorker
from omagent_core.models.llms.base import BaseLLMBackend
from omagent_core.models.llms.openai_gpt import OpenaiGPTLLM
from omagent_core.models.llms.prompt.parser import DictParser
from omagent_core.models.llms.prompt.prompt import PromptTemplate
from omagent_core.utils.registry import registry
from pydantic import Field

@registry.register_worker()
class ExampleVLM(BaseWorker, BaseLLMBackend):
    prompts: List[PromptTemplate] = Field(
        default=[
            PromptTemplate.from_template("You are a helpful assistant. Please output the result in JSON format.", role="system"),
            PromptTemplate.from_template("<tag>user_instruction</tag>, example output: <tag>example_output</tag>, <tag>image</tag>", role="user"), 
        ]
    )
    llm: OpenaiGPTLLM
    output_parser: DictParser = DictParser()
    def _run(self, *args, **kwargs):        
        user_instruction = self.stm(self.workflow_instance_id)["user_instruction"]
        image_url = self.stm(self.workflow_instance_id)["image_cache"]["<image_0>"]
        img = read_image(input_source=image_url)
        example_output = {"objects": [{'xmin': 461.0, 'ymin': 586.0, 'xmax': 1188.0, 'ymax': 1197.0, 'conf': 0.958407998085022, 'label': 'dog'}, {'xmin': 928.0, 'ymin': 522.0, 'xmax': 1493.0, 'ymax': 1079.0, 'conf': 0.82, 'label': 'person'}, {'xmin': 1108.0, 'ymin': 700.0, 'xmax': 1255.0, 'ymax': 888.0, 'conf': 0.10, 'label': 'dog'}, {'xmin': 1180.0, 'ymin': 750.0, 'xmax': 1215.0, 'ymax': 795.0, 'conf': 0.10, 'label': 'dog'}, {'xmin': 461.0, 'ymin': 586.0, 'xmax': 1188.0, 'ymax': 1197.0, 'conf': 0.10, 'label': 'person'}]}).

        chat_completion_res = self.simple_infer(user_instruction=user_instruction, example_output=example_output, image=img)["choices"][0]["message"].get(
            "content"
        )
        return {"last_output": chat_completion_res}

- Please define the prompt to define the tool for example, if you want to use OD, please use VLM to define system as "you are object detector.." in user prompt, define the output format.
In the PromptTemplate.from_template, you need to add "<tag>image</tag>" in the user prompt and add "image" in the input parameters of self.simple_infer.
